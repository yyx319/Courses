{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient methods\n",
    "\n",
    "## The equivalent optimisation problem\n",
    "\n",
    "**Proposition:**\n",
    "\n",
    "If $A$ is symmetric and positive definite the following are equivalent\n",
    "\n",
    "1. $x^*$ satisfies $A x^* = b$\n",
    "2. $f(x^*) \\leq f(x)$ for all vectors $x$ and \n",
    "    $$f(x) = \\frac{1}{2} x^T A x - b^T x$$\n",
    "    \n",
    "---------------------------------------------------------\n",
    "    \n",
    "**Proof.**\n",
    "\n",
    "* Let $x, z$ be arbitrary vectors and $x^* = x - z$\n",
    "* Then one has\n",
    "  $$\\begin{aligned} f(x) &= f(x^* + z) \\\\ &= f(x^*) + z^T (A x^* - b ) + \\frac{1}{2} z^T A z.\\end{aligned}$$\n",
    "  \n",
    "* If $Ax^* = b$ then $f(x)$ is minimal for $z=0$ as $A$ is positive definite\n",
    "* If $f(x^*) \\leq f(x^*+z)$ for all vectors $z$ then $Ax^*-b=0$ as otherwise it would be possible to find\n",
    "    some $z$ for which $f(x^*+z) < f(x^*)$ (choose $z$ such that the second term dominates in size and is\n",
    "    negative)\n",
    "    \n",
    "------------------------------------------------------\n",
    "\n",
    "- optimisation methods to solve the minimisation problem can now be used to compute the solution of \n",
    "    the linear system of equations\n",
    "    \n",
    "- in particular, use methods of the form\n",
    "    $$x^{(k+1)} = x^{(k)} + \\alpha_k d^{(k)}$$\n",
    "    where $\\alpha_k$ is the *stepsize* and $d^{(k)}$ the *search direction*\n",
    "\n",
    "- use optimisation method technology to find best step size and descent direction\n",
    "\n",
    "- We will consider two particular methods, the steepest descent method an the conjugate gradient method\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient method and steepest descent\n",
    "\n",
    "-   For a differentiable function $f : {\\mathbb R}^n \\rightarrow {\\mathbb R}$, its $\\nabla f$ is defined by $$\\nabla f(x)\n",
    "    = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1}(x)& \\dots & \\frac{\\partial f}{\\partial x_n}(x) \\end{bmatrix}^T.$$ \n",
    "    It is known that, for any vector $d$, $$\\frac{d}{d\\alpha} f(x + \\alpha d) = d^T \\nabla f(x+\\alpha d).$$\n",
    "\n",
    "\n",
    "------------------------------------------\n",
    "\n",
    "-   Suppose we have an iterative scheme in which $x^{k+1}$ is given by , i.e. $x_{k+1} = x_k + \\alpha_k d_k$. Then an application of Taylor’s formula gives $$f(x_{k+1}) = f(x_k) + \\alpha_k d_k^T \\nabla f (x_k) \n",
    "    +o(\\alpha_k) \\mbox{ as } \\alpha_k \\rightarrow 0.$$\n",
    "\n",
    "-   Suppose that $\\nabla f(x_k) \\not = 0$. We can then ensure $f(x_{k+1}) < f(x_k)$ (at least for small $\\alpha_k$) if $$d_k^T \\nabla f(x_k)< 0.$$\n",
    "\n",
    "    \n",
    "\n",
    "-   We say that $d_k$ is a *descent direction* if this holds\n",
    "\n",
    "    \n",
    "\n",
    "-   Various different methods can be developed by the choices of descent directions $d_k$ and step sizes $\\alpha_k$.\n",
    "\n",
    "    \n",
    "\n",
    "-   Clearly $d_k:=-\\nabla f(x_k)$ is a descent direction at $x_k$ because $$d_k^T \\nabla f(x_k) = - \\|\\nabla f(x_k)|_2^2 <0.$$ With such choices of $d_k$, it leads to the gradient method $$x_{k+1} = x_k - \\alpha_k \\nabla f(x_k), \\quad k=0,1, \\cdots$$ with suitable choices of $\\alpha_k$.\n",
    "\n",
    "<!-- -->\n",
    "\n",
    "------------------------------------------------\n",
    "\n",
    "-   If the step size $\\alpha_k$ is chosen optimally, in the sense that  $$\\begin{aligned} f(x_k + \\alpha_k d_k ) = \\min_{\\alpha > 0} f( x_k + \\alpha d_k ),\\end{aligned}$$ \n",
    "    the corresponding gradient method is called . It is easy to see that $\\alpha_k$ must then satisfy $$\\left. \\frac{d}{d\\alpha} f(x_k + \\alpha d_k ) \\right|_{\\alpha= \\alpha_k} = 0.$$ This is equivalent to $$d_k^T \\nabla f(x_k + \\alpha_k d_k ) = 0.$$\n",
    "\n",
    "## Example\n",
    "\n",
    "-   We return to the quadratic function $$f(x) = \\frac{1}{2} x^T A x - b^T x.$$ Its gradient is given by $\\nabla f(x) = A x - b$.\n",
    "\n",
    "-   Thus, the corresponding gradient method for solving $Ax=b$ becomes $$x_{k+1} = x_k + \\alpha_k d_k,   \\quad \\mbox{where } d_k =b- A x_k.$$\n",
    "\n",
    "-   If $\\alpha_k$ is chosen optimally, we have $$\\begin{aligned}\n",
    "    0 &= d_k^T \\nabla f(x_k + \\alpha_k d_k ) = d_k^T \\left( A ( x_k + \\alpha_k d_k ) - b \\right)  \\\\\n",
    "      &= d_k^T ( A x_k - b ) + \\alpha_k d_k^T A d_k\\\\\n",
    "      & = -d_k^T d_k + \\alpha_k d_k^T A d_k\\end{aligned}$$ which gives $$\\begin{aligned}\n",
    "    \\alpha_k =\\frac{ d_k^T d_k }{ d_k A d_k}\\end{aligned}$$\n",
    "    \n",
    "---------------------------------------------------------------\n",
    "\n",
    "Summarizing the above, we obtain the following algorithm for solving $A x =b$ with symmetric positive definite matrix $A$.\n",
    "\n",
    "\n",
    "1.  Pick a starting point $x_0 \\in {\\mathbb R}^n$ and set $k:=0$;\n",
    "\n",
    "2.  For $k=0,1,\\cdots$ do\n",
    "\n",
    "    1.  $d_k = b- A x_k$;\n",
    "\n",
    "    2.  $\\alpha_k= \\displaystyle{\\frac{d_k^T d_k}{d_k^T A d_k}}$; \n",
    "\n",
    "    3.  $x_{k+1} =x_k +\\alpha_k d_k$; \n",
    "\n",
    "    4.  $k =k+1$;\n",
    "    \n",
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as nla\n",
    "\n",
    "def A1(x):   # matrix vector product\n",
    "    y = np.zeros(3)\n",
    "    y[0] =  6*x[0] -2*x[1] + 2*x[2]\n",
    "    y[1] = -2*x[0] +5*x[1] +   x[2]\n",
    "    y[2] =  2*x[0] +  x[1] + 4*x[2]\n",
    "    return y\n",
    "x1 = np.array((-.5,1,2))\n",
    "b1  = A1(x1)\n",
    "n = 11\n",
    "xk = np.zeros(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=0 xk=[ 0.  0.  0.] error=2.3\n",
      "k=1 xk=[-0.18169014  1.45352113  1.45352113] error=0.78\n",
      "k=2 xk=[-0.1581733   1.17058415  1.73939771] error=0.46\n",
      "k=3 xk=[-0.36842545  1.18674061  1.77268385] error=0.32\n",
      "k=4 xk=[-0.35833855  1.0711127   1.89252145] error=0.19\n",
      "k=5 xk=[-0.44550883  1.07734604  1.90587306] error=0.13\n",
      "k=6 xk=[-0.44133517  1.02944466  1.95548535] error=0.079\n",
      "k=7 xk=[-0.47743373  1.03203101  1.96101931] error=0.055\n",
      "k=8 xk=[-0.47570526  1.0121939   1.9815653 ] error=0.033\n",
      "k=9 xk=[-0.49065468  1.01326493  1.98385702] error=0.023\n",
      "k=10 xk=[-0.48993888  1.00504983  1.99236568] error=0.014\n"
     ]
    }
   ],
   "source": [
    "for k in range(n):\n",
    "    print(\"k={} xk={} error={:3.2g}\".format(k,xk, nla.norm(xk-x1)))\n",
    "    dk = b1 - A1(xk)\n",
    "    alphak = np.dot(dk,dk)/np.dot(dk,A1(dk))\n",
    "    xk = xk + alphak*dk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The method of steepest descent\n",
    "\n",
    "Consider the system $A x = b$ where $$A = \\left[\\begin{array}{ccc}\n",
    "6 & -2 & 2 \\\\\n",
    "-2 & 5 & 1 \\\\\n",
    "2 & 1 & 4 \\end{array}\\right],  \\quad b = \\left[\\begin{array}{c}\n",
    "-1 \\\\ 8 \\\\ 8\\end{array}\\right] ,$$ which has a solution $x = [ -0.5 , 1 , 2 ]^T$. The table gives the computational result by the method of steepest descent with $x_0 = 0$:\n",
    "\n",
    "|  k  |           | $x_k$      |          |\n",
    "|:---:|:---------:|:----------:|:--------:|\n",
    "|  0  |     0     |      0     |     0    |\n",
    "|  1  | -0.181690 |  1.453521  | 1.453521 |\n",
    "|  2  | -0.158173 |  1.170584  | 1.739398 |\n",
    "|  3  | -0.368425 |  1.186741  | 1.772684 |\n",
    "|  4  | -0.358339 |  1.071113  | 1.892521 |\n",
    "|  5  | -0.445509 |  1.077346  | 1.905873 |\n",
    "|  10 | -0.489939 |  1.005050  | 1.992366 |\n",
    "\n",
    "\n",
    "------------------------------------------------\n",
    "\n",
    "-   The above example shows that the method of steepest descent, like the Jacobi method converges slowly\n",
    "\n",
    "-   By further investigation, one can show that $$(x_{k+1}-x_k)^T (x_k-x_{k-1}) =0\\quad \\mbox{ for } k=1,2, \\cdots$$ for the sequence $\\{x_k\\}$ defined by the method of steepest descent. This means that $\\{x_k\\}$ moves to the exact solution in a zig-zag way which makes the method slow.\n",
    "\n",
    "---------------------------------------------------------------\n",
    "\n",
    "An important question is how fast does this method converge. For a positive definite matrix $A$, the condition number (with respect to the  norm) is given by $$\\kappa(A) = \\frac{\\lambda_{\\max}(A)}{\\lambda_{\\min}(A)}$$ where $\\lambda_{max}(A)$ and $\\lambda_{min}(A)$ are the maximum and minimum eigenvalues of $A$. It is worth noting that these eigenvalues satisfy $$\\lambda_{max}(A) = \\max_{y \\in \\mathbb{R}^m}\n",
    "\\frac{y \\cdot A y }{ \\| y \\|_2 } \\quad \\quad\n",
    "\\lambda_{min}(A) = \\min_{y \\in \\mathbb{R}^m} \\frac{y \\cdot A y }{\n",
    "\\| y \\|_2 }$$ The second ratio is known as the Rayleigh Quotient. Also recall that if $B$ is a symmetric matrix with eigenvalues $\\lambda_1(B),\\ldots,\\lambda_n(B)$ then the  operator norm of $B$ satisfies $$\\| B \\|_2 = \\max_j | \\lambda_j(B) | .$$\n",
    "\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "To obtain an idea of how the gradient method converges, let us consider a steepest descent method with a constant step length. That is, let us consider the iterative method where $\\alpha_k =\n",
    "\\alpha$, a constant, and $$d^{k+1} = r^k = b - A x^k.$$ Then $$x^{k+1} = x^k + \\alpha ( b - A x^k ).$$ Now the exact solution $x$ must also satisfy $$x = x + \\alpha ( b - A x ).$$ Subtracting Equations and leads to the equation $$x^{k+1} - x = ( I - \\alpha A ) (x^k - x ) .$$ \n",
    "\n",
    "----------------------------------------------\n",
    "\n",
    "\n",
    "If we let $e^k$ denote the error at the $k$th step, then we have $e^{k+1} = E e^k$ where the error matrix is given by $E = I - \\alpha A$. For convergence we need $\\| I - \\alpha A \\|_2 < 1$. Consequently, we need $\\max_j | 1 - \\alpha \\lambda_j(A) | < 1$. Since all the eigenvalues are positive, we conclude that this implies that $$\\alpha < \\frac{2}{\\lambda_{\\max}(A)} .$$ Let us choose $\\alpha = 1 / \\lambda_{\\max}(A)$. It turns out that this choice is close to the best choice. Then we have $$\\| I - \\alpha A \\|_2 = 1 -\n",
    "\\frac{\\lambda_{\\min}(A)}{\\lambda_{\\max}(A)} = 1 -\n",
    "\\frac{1}{\\kappa(A)} .$$ This shows that as the condition number increases, the convergence rate decreases. In fact we see that the number of iterations required to obtain a specified accuracy will be proportional to the condition number.\n",
    "\n",
    "--------------------------------------------------------------------\n",
    "\n",
    "The following figure shows an example calculation based on the steepest descent method. Observe that all of the steps only go in one of two directions. What if we could combine the steps so we only took one big step in each direction? Then the solution would be found in two steps. The Conjugate gradient method tries to achieve that goal.\n",
    "\n",
    "-------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def A(x):\n",
    "    y1 = 1.3*x[0] -   x[1]\n",
    "    y2 = -x[0] +  1.1*x[1]\n",
    "    return np.array((y1,y2))\n",
    "xe = np.array((2.0,1.0))\n",
    "b = A(xe)\n",
    "\n",
    "import pylab as pl\n",
    "\n",
    "n = 51\n",
    "xk = np.zeros(2)\n",
    "xg = np.zeros((2,n))\n",
    "for k in range(n):\n",
    "    xg[:,k] = xk\n",
    "    dk = b - A(xk)\n",
    "    alphak = np.dot(dk,dk)/np.dot(dk,A(dk))\n",
    "    xk = xk + alphak*dk\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgwAAAFyCAYAAACZRoIjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8e75fcaef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pl.title(\"path to solution\")\n",
    "pl.axis([0,2.5,-1,2]);pl.axis('equal');\n",
    "pl.plot(xg[0,:],xg[1,:]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your gradient descent\n",
    "# explore some 2 x 2 matrices, use contour plots to visualise isolines or countour lines of f(x)\n",
    "# plot how gradient descent moves through the 2d space and reduces f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjugate Gradient Method\n",
    "\n",
    "-   The method of steepest descent has a tendency to zig-zag into the solution. This can make the method convergent slowly.\n",
    "\n",
    "-   We next consider the which is a fast algorithm; it was proposed by Hestens and Stiefel in 1950.\n",
    "\n",
    "-   We start with the description of a general class of iterative methods — .\n",
    "\n",
    "A set of nonzero vectors $\\{d_0, d_1, \\cdots, d_{n-1}\\}$ in ${\\mathbb R}^n$ is called (or ) if $$d_i^T A d_j = 0\\quad \\mbox{ for all } i\\ne j.$$\n",
    "\n",
    "One can check that any nonzero $A$-conjugate vectors $\\{d_0, \\cdots, d_{n-1}\\}$ are linearly independent and hence form a basis of ${\\mathbb R}^n$.\n",
    "\n",
    "--------------------------------------------------\n",
    "\n",
    "-   Given a set of nonzero $A$-conjugate vectors $\\{d_0, \\cdots, d_{n-1}\\}$ in ${\\mathbb R}^n$. A is to find the minimizer of $$f(x) = \\frac{1}{2} x^T A x - b^T x$$ in $n$ steps by successively minimizing $f(x)$ along the individual directions $d_i$.\n",
    "\n",
    "-   To be more precise, let $x_0\\in {\\mathbb R}^n$ be a starting point, it generates a sequence $\\{x_k\\}$ by setting $$x_{k+1} =x_k +\\alpha_k d_k,$$ where $\\alpha_k$ is chosen by $$f(x_k +\\alpha_k d_k) = \\min_{\\alpha\\in {\\mathbb R}} f(x_k +\\alpha d_k).$$\n",
    "\n",
    "-   $\\alpha_k$ can be determined by solving the equation $$\\frac{d}{d\\alpha} f(x_k +\\alpha d_k) =0.$$\n",
    "\n",
    "<!-- -->\n",
    "\n",
    "-----------------------------------------------------------------------------\n",
    "\n",
    "-   Note that \n",
    "$$\\begin{aligned}\n",
    "    f(x_k+\\alpha d_k)\n",
    "    & = \\frac{1}{2} (x_k + \\alpha d_k)^T A (x_k +\\alpha d_k) - b^T (x_k +\\alpha d_k)\\\\\n",
    "    & = \\frac{1}{2} \\alpha^2 d_k^T A d_k + \\alpha d_k^T A x_k + \\frac{1}{2} x_k^T A x_k \n",
    "    -b^T x_k - \\alpha d_k^T b\\end{aligned}$$ Therefore $$\\begin{aligned}\n",
    "    \\frac{d}{d\\alpha} f(x_k +\\alpha d_k) \n",
    "    & = \\alpha d_k^T A d_k + d_k^T A x_k -d_k^T b\\\\\n",
    "    & = \\alpha d_k^T A d_k + d_k^T \\left(A x_k -b\\right).\\end{aligned}$$\n",
    "\n",
    "-   Thus $\\alpha_k$ satisfies the equation $$\\alpha_k d_k^T A d_k + d_k^T \\left(A x_k -b\\right) =0.$$ Let $r_k = A x_k -b$. Then $$\\alpha_k = -\\frac{d_k^T r_k}{d_k^T A d_k}.$$\n",
    "\n",
    "<!-- -->\n",
    "\n",
    "----------------------------------------------------------------------------------\n",
    "\n",
    "-   Consequently, a conjugate direction method can be formed by repeating the steps $$\\begin{aligned}\n",
    "    \\begin{split}\n",
    "    r_k & = A x_k -b,\\\\\n",
    "    \\alpha_k & = -\\frac{d_k^T r_k}{d_k^T A d_k},\\\\\n",
    "    x_{k+1} & = x_k +\\alpha_k d_k\n",
    "    \\end{split}\\end{aligned}$$ from an initial guess $x_0$, once a set of nonzero $A$-conjugate vectors $\\{d_0, \\cdots, d_{n-1}\\}$ are known.\n",
    "\n",
    "-   How to find a set of nonzero $A$-conjugate vectors $\\{d_0, \\cdots, d_{n-1}\\}$ in a computationally efficient way?\n",
    "\n",
    "-   is a conjugate direction method with the conjugate vectors constructed successively during computation.\n",
    "\n",
    "<!-- -->\n",
    "\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "-   It starts with $$r_0 = A x_0-b \\quad \\mbox{and} \\quad d_0 =-r_0$$ and construct each new $d_{k+1}$ as the linear combination of $r_{k+1}$ and $d_k$ of the form $$d_{k+1} =-r_{k+1} +\\beta_{k+1} d_k,$$ where $\\beta_{k+1}$ is chosen so that $d_{k+1}$ and $d_k$ are $A$-conjugate, i.e. $$0=d_{k+1}^T A d_k =-r_{k+1}^T A d_k +\\beta_{k+1} d_k^T A d_k$$ and hence $$\\beta_{k+1} =\\frac{r_{k+1}^T A d_k}{d_k^T A d_k}.$$\n",
    "\n",
    "-   Combining this construction of $d_{k+1}$ with (\\[3.24.2\\]), it leads to the following conjugate gradient method.\n",
    "\n",
    "\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "1.  Pick a starting point $x_0 \\in {\\mathbb R}^n$;\n",
    "\n",
    "2.  Set $r_0:= A x_0-b$, $d_0:=-r_0$, and $k:=0$;\n",
    "\n",
    "3.  While $r_k\\ne 0$ do\n",
    "\n",
    "    1.  $\\alpha_k= \\displaystyle{-\\frac{r_k^T d_k}{d_k^T A d_k}}$; \n",
    "\n",
    "    2.  $x_{k+1} =x_k +\\alpha_k d_k$; \n",
    "\n",
    "    3.  $r_{k+1} = A x_{k+1} -b$; \n",
    "\n",
    "    4.  $\\beta_{k+1}:= \\displaystyle{\\frac{r_{k+1}^T A d_k}{d_k^T A d_k}}$; \n",
    "\n",
    "    5.  $d_{k+1}:= -r_{k+1} +\\beta_{k+1} d_k$; \n",
    "\n",
    "    6.  $k =k+1$;\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "-   One can show that the vectors $\\{d_0, d_1, \\cdots, d_{n-1}\\}$ constructed by the algorithm are $A$-conjugate.\n",
    "\n",
    "-   The sequence $\\{x_k\\}$ generated by the conjugate gradient method converges to the solution $x^*$ of $A x=b$ in at most $n$ steps.\n",
    "\n",
    "Consider the linear system $A x =b$ of size $n=600$, where $$A = \\left[\\begin{array}{ccccc}\n",
    "4 &     -1  &        &        & \\\\\n",
    "-1  &    4  &  -1     &        & \\\\\n",
    "   & \\ddots & \\ddots & \\ddots & \\\\\n",
    "   &        &  -1     &  4    &  -1\\\\\n",
    "   &        &        &   -1    & 4\n",
    "\\end{array}\\right],\n",
    "\\qquad\n",
    "b = \\left[\\begin{array}{ccccc}\n",
    "1 \\\\ 2 \\\\ 3 \\\\ \\vdots \\\\ n\n",
    "\\end{array}\\right].$$ \n",
    "\n",
    "-------------------------------------------------------------------------\n",
    "\n",
    "<!-- end of notebook NB2.6 after next block -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# some other data\n",
    "def A2(x): # the matrix vector product\n",
    "    y = 4*x\n",
    "    y[1:]  -= x[:-1]\n",
    "    y[:-1] -= x[1:]\n",
    "    return y\n",
    "x2 = (np.linspace(0,1,102)*(1-np.linspace(0,1,102)))[1:-1]*102**2\n",
    "b2 = A2(x2)\n",
    "\n",
    "A = A1\n",
    "b = b1\n",
    "xex = x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "-----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=0 xk=[ 0.  0.  0.] error=2.3\n",
      "k=1 xk=[-0.18169014  1.45352113  1.45352113] error=0.78\n",
      "k=2 xk=[-0.16248694  1.20250784  1.78683386] error=0.45\n",
      "k=3 xk=[-0.5  1.   2. ] error=3.1e-16\n",
      "k=4 xk=[-0.5  1.   2. ] error=3.1e-16\n"
     ]
    }
   ],
   "source": [
    "xk = np.zeros(len(b))\n",
    "rk = A(xk)-b\n",
    "dk = -rk\n",
    "for k in range(5):\n",
    "    print(\"k={} xk={} error={:3.2g}\".format(k,xk, nla.norm(xk-xex)))\n",
    "    Adk = A(dk)\n",
    "    dkAdk = np.dot(dk,A(dk))\n",
    "    alphak = -np.dot(rk,dk)/dkAdk\n",
    "    xk = xk + alphak*dk\n",
    "    rk = A(xk) - b\n",
    "    betak = np.dot(rk,A(dk))/dkAdk\n",
    "    dk = -rk + betak*dk"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
